{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74dcc21a-5489-47d5-94ae-c53cf6d1dce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e62c0653-2ff9-48a7-8d7a-560cf2f708d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Friedman test function defined!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def friedman_test(data):\n",
    "    \"\"\"\n",
    "    Compute Friedman test statistic.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : numpy array of shape (n_datasets, n_algorithms)\n",
    "        Performance scores for each algorithm on each dataset (fold)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    chi2_f : float\n",
    "        Friedman test statistic\n",
    "    p_value : float\n",
    "        P-value\n",
    "    avg_ranks : numpy array\n",
    "        Average ranks for each algorithm\n",
    "    \"\"\"\n",
    "    n_datasets, k = data.shape\n",
    "    \n",
    "    # Rank algorithms for each dataset (lower is better for time, higher for performance)\n",
    "    ranks = np.zeros_like(data)\n",
    "    for i in range(n_datasets):\n",
    "        # For training time: lower is better (rank 1 = fastest)\n",
    "        # For accuracy/F-measure: higher is better (rank 1 = best)\n",
    "        ranks[i] = stats.rankdata(-data[i])  # Negative for descending order\n",
    "    \n",
    "    # Average ranks across datasets\n",
    "    avg_ranks = np.mean(ranks, axis=0)\n",
    "    \n",
    "    # Friedman statistic\n",
    "    rank_sum_sq = np.sum(avg_ranks ** 2)\n",
    "    chi2_f = (12 * n_datasets / (k * (k + 1))) * (rank_sum_sq - k * ((k + 1) ** 2) / 4)\n",
    "    \n",
    "    # Degrees of freedom\n",
    "    df = k - 1\n",
    "    \n",
    "    # P-value from chi-square distribution\n",
    "    p_value = 1 - stats.chi2.cdf(chi2_f, df)\n",
    "    \n",
    "    return chi2_f, p_value, avg_ranks\n",
    "\n",
    "print(\"Friedman test function defined!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6da4cde-c33f-4c04-93db-d5a02613543a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nemenyi test function defined!\n"
     ]
    }
   ],
   "source": [
    "def nemenyi_critical_difference(n_datasets, k, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Calculate critical difference for Nemenyi post-hoc test.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_datasets : int\n",
    "        Number of datasets (folds)\n",
    "    k : int\n",
    "        Number of algorithms\n",
    "    alpha : float\n",
    "        Significance level (default 0.05)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    cd : float\n",
    "        Critical difference\n",
    "    \"\"\"\n",
    "    # Critical values for Nemenyi test (studentized range statistic)\n",
    "    # For alpha=0.05\n",
    "    q_alpha_values = {\n",
    "        2: 1.960, 3: 2.343, 4: 2.569, 5: 2.728, 6: 2.850,\n",
    "        7: 2.949, 8: 3.031, 9: 3.102, 10: 3.164\n",
    "    }\n",
    "    \n",
    "    if k not in q_alpha_values:\n",
    "        raise ValueError(f\"Number of algorithms {k} not supported. Use 2-10 algorithms.\")\n",
    "    \n",
    "    q_alpha = q_alpha_values[k]\n",
    "    \n",
    "    # Critical difference formula\n",
    "    cd = q_alpha * np.sqrt((k * (k + 1)) / (6 * n_datasets))\n",
    "    \n",
    "    return cd\n",
    "\n",
    "print(\"Nemenyi test function defined!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b79939a-c848-42fb-a593-283065df0850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loading function defined!\n"
     ]
    }
   ],
   "source": [
    "def load_spambase_data(filepath='spambase.data'):\n",
    "    \"\"\"\n",
    "    Load and preprocess the Spambase dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    filepath : str\n",
    "        Path to the spambase.data file\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    X : numpy array\n",
    "        Feature matrix\n",
    "    y : numpy array\n",
    "        Target labels\n",
    "    \"\"\"\n",
    "    # Load data\n",
    "    data = pd.read_csv(filepath, header=None)\n",
    "    \n",
    "    # Separate features and target\n",
    "    X = data.iloc[:, :-1].values\n",
    "    y = data.iloc[:, -1].values\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "print(\"Data loading function defined!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6cea38e-4759-44c2-8d11-3df6853a0783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Dataset loaded successfully!\n",
      "  - Number of samples: 4601\n",
      "  - Number of features: 57\n",
      "  - Class distribution: [2788 1813]\n",
      "    * Non-spam (0): 2788\n",
      "    * Spam (1): 1813\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "try:\n",
    "    X, y = load_spambase_data('spambase.data')\n",
    "    print(f\"✓ Dataset loaded successfully!\")\n",
    "    print(f\"  - Number of samples: {X.shape[0]}\")\n",
    "    print(f\"  - Number of features: {X.shape[1]}\")\n",
    "    print(f\"  - Class distribution: {np.bincount(y.astype(int))}\")\n",
    "    print(f\"    * Non-spam (0): {np.sum(y == 0)}\")\n",
    "    print(f\"    * Spam (1): {np.sum(y == 1)}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"✗ ERROR: spambase.data not found!\")\n",
    "    print(\"  Please download from: https://archive.ics.uci.edu/ml/datasets/Spambase\")\n",
    "    print(\"  and place it in the same directory as this notebook.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0547f84b-a6e7-4a51-9bc9-9fbe034fed17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation function defined!\n"
     ]
    }
   ],
   "source": [
    "def evaluate_algorithms(X, y, n_folds=10):\n",
    "    \"\"\"\n",
    "    Evaluate three algorithms using stratified 10-fold cross-validation.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : numpy array\n",
    "        Feature matrix\n",
    "    y : numpy array\n",
    "        Target labels\n",
    "    n_folds : int\n",
    "        Number of folds for cross-validation\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    results : dict\n",
    "        Dictionary containing results for each metric and algorithm\n",
    "    \"\"\"\n",
    "    # Initialize algorithms\n",
    "    algorithms = {\n",
    "        'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "        'Naive Bayes': GaussianNB(),\n",
    "        'SVM': SVC(random_state=42)\n",
    "    }\n",
    "    \n",
    "    # Initialize result storage\n",
    "    results = {\n",
    "        'training_time': {name: [] for name in algorithms.keys()},\n",
    "        'accuracy': {name: [] for name in algorithms.keys()},\n",
    "        'f_measure': {name: [] for name in algorithms.keys()}\n",
    "    }\n",
    "    \n",
    "    # Stratified K-Fold\n",
    "    skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "    \n",
    "    fold_num = 1\n",
    "    for train_idx, test_idx in skf.split(X, y):\n",
    "        print(f\"Processing Fold {fold_num}/{n_folds}...\", end=' ')\n",
    "        \n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "        \n",
    "        # Standardize features\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        \n",
    "        # Evaluate each algorithm\n",
    "        for name, clf in algorithms.items():\n",
    "            # Measure training time\n",
    "            start_time = time.time()\n",
    "            clf.fit(X_train_scaled, y_train)\n",
    "            training_time = time.time() - start_time\n",
    "            \n",
    "            # Make predictions\n",
    "            y_pred = clf.predict(X_test_scaled)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            acc = accuracy_score(y_test, y_pred)\n",
    "            f1 = f1_score(y_test, y_pred, average='binary')\n",
    "            \n",
    "            # Store results\n",
    "            results['training_time'][name].append(training_time)\n",
    "            results['accuracy'][name].append(acc)\n",
    "            results['f_measure'][name].append(f1)\n",
    "        \n",
    "        print(\"✓\")\n",
    "        fold_num += 1\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"Evaluation function defined!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "241cdb56-a5e4-421d-8fed-0dd591b6faab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "RUNNING 10-FOLD CROSS-VALIDATION\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"RUNNING 10-FOLD CROSS-VALIDATION\")\n",
    "print(\"=\"*80)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20b0d0b5-c8f5-4f0c-8160-68f7a2da2bf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Fold 1/10... ✓\n",
      "Processing Fold 2/10... ✓\n",
      "Processing Fold 3/10... ✓\n",
      "Processing Fold 4/10... ✓\n",
      "Processing Fold 5/10... ✓\n",
      "Processing Fold 6/10... ✓\n",
      "Processing Fold 7/10... ✓\n",
      "Processing Fold 8/10... ✓\n",
      "Processing Fold 9/10... ✓\n",
      "Processing Fold 10/10... ✓\n",
      "\n",
      "✓ All experiments completed successfully!\n"
     ]
    }
   ],
   "source": [
    "results = evaluate_algorithms(X, y)\n",
    "\n",
    "print()\n",
    "print(\"✓ All experiments completed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "375c6b42-cf54-4a88-baff-79206d4458ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_results_table(results, metric_name):\n",
    "    \"\"\"\n",
    "    Create results table similar to Example 12.4 format.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(results)\n",
    "    df.index = [f'Fold {i+1}' for i in range(len(df))]\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76fa6e1e-c4d7-4e91-8c58-e70fae90c54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_friedman_results(chi2_f, p_value, avg_ranks, algorithm_names, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Display Friedman test results in Example 12.8 format.\n",
    "    \"\"\"\n",
    "    print(f\"\\nFriedman Test Results:\")\n",
    "    print(f\"  Friedman Statistic (χ²_F): {chi2_f:.4f}\")\n",
    "    print(f\"  P-value: {p_value:.6f}\")\n",
    "    print(f\"\\nAverage Ranks:\")\n",
    "    for name, rank in zip(algorithm_names, avg_ranks):\n",
    "        print(f\"  {name}: {rank:.2f}\")\n",
    "    \n",
    "    # Determine significance\n",
    "    is_significant = p_value < alpha\n",
    "    \n",
    "    print(f\"\\nSignificance Test (α = {alpha}):\")\n",
    "    if is_significant:\n",
    "        print(f\"  Result: SIGNIFICANT (p-value = {p_value:.6f} < {alpha})\")\n",
    "        print(f\"  Conclusion: Average ranks display significant differences.\")\n",
    "    else:\n",
    "        print(f\"  Result: NOT SIGNIFICANT (p-value = {p_value:.6f} ≥ {alpha})\")\n",
    "        print(f\"  Conclusion: No significant differences between average ranks.\")\n",
    "    \n",
    "    return is_significant\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7932fee-b7ec-4abb-b032-12362b0581e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Display functions defined!\n"
     ]
    }
   ],
   "source": [
    "def display_nemenyi_results(avg_ranks, algorithm_names, cd):\n",
    "    \"\"\"\n",
    "    Display Nemenyi post-hoc test results.\n",
    "    \"\"\"\n",
    "    print(f\"\\nNemenyi Post-hoc Test:\")\n",
    "    print(f\"  Critical Difference (CD): {cd:.4f}\")\n",
    "    print(f\"\\n  Pairwise Comparisons:\")\n",
    "    \n",
    "    for i in range(len(algorithm_names)):\n",
    "        for j in range(i+1, len(algorithm_names)):\n",
    "            rank_diff = abs(avg_ranks[i] - avg_ranks[j])\n",
    "            is_diff = rank_diff > cd\n",
    "            status = \"SIGNIFICANTLY DIFFERENT\" if is_diff else \"Not significantly different\"\n",
    "            symbol = \">\" if is_diff else \"≤\"\n",
    "            print(f\"    {algorithm_names[i]} vs {algorithm_names[j]}:\")\n",
    "            print(f\"      |{avg_ranks[i]:.2f} - {avg_ranks[j]:.2f}| = {rank_diff:.4f} {symbol} {cd:.4f}\")\n",
    "            print(f\"      → {status}\")\n",
    "\n",
    "print(\"Display functions defined!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "34a99d9f-9164-405e-b40e-017e66eaceae",
   "metadata": {},
   "outputs": [



